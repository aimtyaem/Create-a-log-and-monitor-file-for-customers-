[
{
	"uri": "/",
	"title": "OpenSearch Log Analytics Workshop",
	"tags": [],
	"description": "",
	"content": "OpenSearch Log Analytics Workshop This is a workshops and resources for using AWS OpenSearch for Log Analytics.\nIn the workshop OpenSearch Log Analytics you will learn how to perform log analytics via. AWS OpenSearch. You will explore the basics of ingesting, analyzing and visualizing data in OpenSearch.\nIn the workshop CloudWatch Log Collection you will learn how to send logs - in realtime - from AWS CloudWatch service to AWS OpenSearch. Once in OpenSearch you can analyize your CloudWatch logs and help identify error and issues.\n"
},
{
	"uri": "/collect-log-cloud-watch/1_getting_started.html",
	"title": "1. Getting started",
	"tags": [],
	"description": "",
	"content": "An AWS account and access to the AWS web console is required to complete this workshop.\nIf you are are attending an AWS hosted event (re:Invent, Loft, Immersion Day, or any other event hosted by an AWS employee) follow the instructions provided at AWS Event\nIf you are running this workshop on your own follow the instructions provided at Self Paced\n"
},
{
	"uri": "/open-search-log-analytics/1_getting_started.html",
	"title": "1. Getting started",
	"tags": [],
	"description": "",
	"content": "An AWS account and access to the AWS web console is required to complete this workshop.\nIf you are are attending an AWS hosted event (re:Invent, Loft, Immersion Day, or any other event hosted by an AWS employee) follow the instructions provided at AWS Event\nIf you are running this workshop on your own follow the instructions provided at Self Paced\n"
},
{
	"uri": "/open-search-log-analytics/",
	"title": "1_OpenSearch Log Analytics",
	"tags": [],
	"description": "",
	"content": "OpenSearch Log Analytics Workshop This workshop is an introduction to log analytics with AWS OpenSearch. OpenSearch allows for full text search on logs indexed by the service. In this workshop we use sample Spark, Hadoop, HDFS and ZooKeeper logs to learn about log analytics with AWS OpenSearch\nIn this workshop we will implement the following architecture\nThe architecture uses a Python application run from a Cloud9 development enviorment to simulate a log producer. The application will send 8,000 logs that we will later analyze. The Python application will send the logs to Kinesis Data Firehose. Kinesis Data Firehose will ingest the log into OpenSearch. Once the logs are ingested we will use OpenSearch Dashboard to analyze our logs\nWhen you are ready to being the lab navigate to Getting started\n"
},
{
	"uri": "/collect-log-cloud-watch/",
	"title": "2. CloudWatch Log Collection",
	"tags": [],
	"description": "",
	"content": "Collect AWS CloudWarch Logs for OpenSearch "
},
{
	"uri": "/collect-log-cloud-watch/2_enviorment_set_up.html",
	"title": "2. Environment Set Up",
	"tags": [],
	"description": "",
	"content": "We need to deploy a few services and configure our AWS enviorment before we can get started.\nWe will need to complete the following set up steps\n Create an OpenSearch Domain Create an IAM Role Map IAM Role to OpenSearch Role Deploy Glue Jobs to Generate CloudWatch Logs  Follow the instructions below for each step\nCreate an OpenSearch Domain  Go to the OpenSearch Console Click on Create domain  Enter the name workshop-domain for the OpenSearch Domain Under the deployment type section, select Development and testing Under the network section, select Public access Under the fine-grained access control section select Create master user Enter and username and password. Copy down the user name and password. We will need these later in the workshop Leave all other settings at the default selections Click on Create  It will take approximately 5 - 10 minutes for your OpenSearch domain to be created. Upon sucssful creation you will see your OpenSearch domain status is active\nDo not proceed to the next step until you confirm that your domain status is active\nCreate an IAM Role  Go to the IAM Console Clik on Roles  Click on Create role  Under the choose a use case click on Lambda and then click on Next:Permissions  Search for AdministratorAccess and click on the box next to the AdministratorAccess policies  Note for the purposes of this workshop we will give the IAM role AdministratorAccess. In a production enviorment you should scope down the premissions give to the IAM role\nClick on Next: Tags Click on Next: Review . There is no need to change anything on the Add tags page Name the role workshop-role Click on Create role  Back on the IAM Roles page search for and select the workshop-role you just created  On the summary page copy down the Role ARN. We will need this later in the workshop  Click on the Trust relationships tab Click on the Edit trust relationship  Replace the service section of the JSON with the following  \u0026quot;Service\u0026quot;: [ \u0026quot;lambda.amazonaws.com\u0026quot;, \u0026quot;glue.amazonaws.com\u0026quot; ] Your trust relationship policy document should look like the following\nClick on Update Trust Policy  Map IAM Role to OpenSearch Role  Go to the OpenSearch Console Click on the workshop-domain OpenSearch domain you created earlier Click on the OpenSearch Dashboard URL. This should open the URL in a web browser window  You will be prompted to log in. Using the user name and password you created during the OpenSearch deployment, log in If an additonal pop up window is present after login asking about data upload click on Explore on my own If an additonal pop up windows is present asking you to select your tenant select Global and click on Confirm  You should now see a window that looks like this\nClick on and expand the hamburger menu on the side bar of the OpenSearch home page Under the OpenSearch Plugins section click on Security  On the security page click on Roles from the left hand menu  On the roles page search for and click on all_access  On the all_access role page click on Mapped users Under the mapped users page click on Manage mapping  Under the backend roles section enter the ARN that you copied down earlier for the IAM workshop-role that you created Click on Map  Deploy Glue Jobs to Generate CloudWatch Logs CloudWatch provides log collection for AWS services. In order to generate logs that we can use in this workshop we will create two simple AWS Glue Jobs. We can run this jobs to produce logs. Subseqently we can analyze these logs in OpenSearch\n Go to the Glue Console On the left hand menu click on Jobs  On the jobs page click on Add job  Enter glue_job_success for the name of the job For the IAM tole select workshop-role Under the this job run section select A new script to be authored by you  Under the monitoring options click on Job metrics and Continuous logging  Under the security configuration, script libraries, and job parameters (optional) set the max concurrency to 10   Leave all other settings at the default selections and click on Next at the bottom of the page\n  On the connections page leave all of the setting at the defaults and click on Save job and edit script\n  In the edit job window copy and past the following code into the editing window  print(\u0026quot;Run this job to generate Glue logs in CloudWatch. We will later analyze these with OpenSearch\u0026quot;) Click on Save Click on Run Repeat steps 1 - 13 again. However this time name the Glue job glue_job_error and use the following code in the job  print(\u0026quot;Run this will create an error. We will later analyze this error with OpenSearch This code has incorrect syntax unpurpose. When you run this job it will create an error in CloudWatch that we can look at later with OpenSearch\nYou have complted the set up for your AWS enviorment! When you are ready lets begin the next step Send Log Data to OpenSearch\n"
},
{
	"uri": "/open-search-log-analytics/2_enviorment_set_up.html",
	"title": "2. Environment Set Up",
	"tags": [],
	"description": "",
	"content": "We will need to deploy a few services and configure our AWS environment before we can get started with log analytics.\nWe will need to compelete the following set up steps\n Create an OpenSearch Domain Create a Kinesis Firehose Configure Identity Access Management Premisions  Follow the instructions below for each step\nCreate an OpenSearch Domain  Go to the OpenSearch Console Click on Create domain  Enter the name workshop-domain for the OpenSearch Domain Under the deployment type section, select Development and testing Under the network section, select Public access Under the fine-grained access control section select Create master user Enter and username and password. Copy down the user name and password. We will need these later in the workshop Leave all other settings at the default selections Click on Create  It will take approximately 5 - 10 minutes for your OpenSearch domain to be created. Upon sucssful creation you will see your OpenSearch domain status is active\nDo not proceed to the next step until you confirm that your domain status is active\nCreate a Kinesis Firehose  Go to the Kinesis Firehose Console Click on Create delivery stream  Under the choose source and destination section for the source, select Direct PUT for the desintation select Amazon OpenSearch Service Under the delivery stream name section name the stream workshop-firehose  Under the destination settings for the OpenSearch service domain, click on Browse and select the OpenSearch domain workshop-domain this is the OpenSearch domain we created in the previous step Name the index workshop-log Select Every hour for the Index rotation. This will produce a next index every hour  Expand the Buffer hints section Adjust the buffer interval to 60 seconds. This will write data from Kinesis Firehose to OpenSearch every 60 seconds  Under the backup settings under the S3 backup bucket click on Create. This will (in a new browser window) open the create S3 bucket web page  On the create bucket page provide a bucket name. You may name the bucket any valid name Click on Create bucket at the bottom of the page. Leaving all of the S3 settings the the default selections Return the browser window that we were using to configure our Kinesis Firehose and under the backup setting section for the S3 backup bucket, click on Browse Select the bucket you just created. If you do not see the bucket listed click on the small refresh button in the top right corner of the window that pops us when you click on the browse button  At the bottom of the page click on Create delivery stream leave all other settings at the default selections  Configure Identity Access Management (IAM) Premisions In the first step we created an OpenSearch domain. We can send logs to the OpenSearch domin via. the Kinesis Data Firehose we just created\nHowever before we can start to send sample log data to OpenSearch (via Kinesis Data Firehose) we need to configure premissions in IAM and in OpenSearch\nAdjust OpenSearch Access Policy  Go to the OpenSearch Console Click on the workshop-domain OpenSearch domain you created earlier  Click on Security configuration Under the security configuration window click on Edit  Naviage to the access policy section of the edit security configuration window Adjust the JSON access policy to switch the Deny to an Allow  Click on Save changes at the bottom of the page  Map IAM Role with OpenSeach Role  Go to the OpenSearch Console Click on the workshop-domain OpenSearch domain you created earlier Click on the OpenSearch Dashboard URL. This should open the URL in a web browser window  You will be prompted to log in. Using the user name and password you created during the OpenSearch deployment, log in If an additonal pop up window is present after login asking about data upload click on Explore on my own If an additonal pop up windows is present asking you to select your tenant select Global and click on Confirm  You should now see a window that looks like this\nClick on and expand the hamburger menu on the side bar of the OpenSearch home page Under the OpenSearch Plugins section click on Security  On the security page click on Roles from the left hand menu  On the roles page search for and click on all_access  On the all_access role page click on Mapped users Under the mapped users page click on Manage mapping  On the manage mapping page we need to map the IAM role the is used by Kinesis Data Firehose to the all_access OpenSearch role. This will give Kinesis Firehose the premissions it need to create and update indexes.\nFor the purposes of this lab we will give Kinesis Firehose all_access in OpenSearch. In a production environment it is recomended to scope down the premisions Kinesis Firehose has within OpenSearch\nWe need to find the ARN of the IAM role Kinesis Firehose is using. Keeping the Manage mapping page open, navigate to a new tab and:\nGo to the Kinesis Firehose Console Click on the workshop-firehose listed. This is the Kinesis Data Fire hose we created earlier Click on the Configuration section  On the configuration page scroll down to the premissions section Click on the IAM role  This will open a new window in your web browser. Copy down the ARN of the IAM role  Navigate back to the OpenSearch map user tab Enter the ARN we copied in step 18 and paste it in the backend roles section of OpenSearch console page Click on Map  Our AWS environment set up is now complete! When you are ready lets begin the next step Send Log Data to Kinesis Fire Hose\n"
},
{
	"uri": "/open-search-log-analytics/3_send_log_data_to_kinesis_fire_hose.html",
	"title": "3. Send Log Data to Kinesis Fire Hose",
	"tags": [],
	"description": "",
	"content": "We need to send sample log data to Kinesis Data Firehose which in turn will send the data to OpenSearch.\nWe will run a python application that will send a few different types of log data to OpenSearch. We will run the sample Python application in a Cloud9\nCreate a Cloud9 environment\n Go to the Cloud9 Console Click on Create environment  Under the name environment section enter log_producer for the name Click on Next step Leave all of the settings at the default selections Click on Next step Click on Create environment  After the Cloud9 envriorment is created your broswer will automaticlly be redirected to the Cloud9 console\nRun a Python Application from Cloud9\nWith in the Cloud9 console running the following commands in the console section of the Cloud9 environment\n wget https://sharkech-public.s3.amazonaws.com/opensearch-log-analytics/data-producer/Log_Producer_Desktop.py  The image below highlights were to run the commands. Run all of the commands in order\nmkdir sample_logs cd sample_logs wget https://sharkech-public.s3.amazonaws.com/opensearch-log-analytics/data-producer/sample_logs/hadoop.txt wget https://sharkech-public.s3.amazonaws.com/opensearch-log-analytics/data-producer/sample_logs/hdfs.txt wget https://sharkech-public.s3.amazonaws.com/opensearch-log-analytics/data-producer/sample_logs/spark.txt wget https://sharkech-public.s3.amazonaws.com/opensearch-log-analytics/data-producer/sample_logs/zoo_keeper.txt cd .. pip install boto3 python Log_Producer_Desktop.py  These commands download the sample log data. They also download and configure the python script that will send the sample log data to Kinesis Data Firehose.\nUpon running the last command you should be messages appearing in the Cloud9 console indicating the logs are being sent.\nLeave this broswer window open. This way the python application continues to run and send data.\nWhen you are ready move on to the next step Visualize and Analyze\n"
},
{
	"uri": "/collect-log-cloud-watch/3_send_log_data_to_opensearch.html",
	"title": "3. Send Log Data to OpenSearch",
	"tags": [],
	"description": "",
	"content": "We have now create an OpenSearch domain, configured the required IAM and OpenSearch premissions and we created two Glue jobs that we can run. One Glue job that completes sucssfully and another which creates an error.\nAll of the log information for the Glue jobs is collected in CloudWatch. Lets send the data - in realtime - from CloudWatch to OpenSearch\nWe can complete this in the following steps\n Set up CloudWatch to OpenSearch Lambda Function(s) Re-run the Glue Jobs to Create additonal CloudWatch Logs  Set up CloudWatch to OpenSearch Lambda Function(s)  Navigate to the CloudWatch Console On the left hand menu click on Log groups  On the log groups page of CloudWatch you should see a few log groups which begin with /aws-glue/\nClick on the log group aws-glue/jobs/error Click on the Subscription filters tab Click on the Create drop down Click on the Create Amazon OpenSearch Service subscription filter  On the Create Amazon OpenSearch Service subscription filter page, under the choose destination click on This account From the drop down select the workshop-domain  Under the Lambda Function section select the IAM role workshop-role that we created earlier  Under the configure log format and filters select JSON For the subscription filter pattern enter \u0026quot; \u0026quot; For the subscription filter name all log  Click on Start streaming  The steps you just completed created a lambda function that will send CloudWatch logs to OpenSearch in realtime.\nNow that we have compelted this process for the \t/aws-glue/jobs/error log group Navigate back to the CloudWatch Console and repeat steps 1 - 13 for log groups \t/aws-glue/jobs/logs-v2 and /aws-glue/jobs/output\nRe-run the Glue Jobs to Create additonal CloudWatch Logs  Go to the Glue Console On the left hand menu click on Jobs  Click on the check box next to the glue_job_success job Click on the Action drop down Click on the Run job button  Repeat step 3 - 4 for of the jobs you created earlier ie. glue_job_success and glue_job_error. Run each job a few times. The goal is to generate some CloudWatch logs that will be sent to OpenSearch.\nYou can view the job run history for a job by click on the check box next to job name and viewing the history tab\nEnsure that you have at least 3 runs completed for each job. The job status for the glue_job_success should be Succeeded and the job status for the glue_job_error should be Failed\nWhen you have at least 3 completed job runs for each job begin the next step Visualize and Analyze\n"
},
{
	"uri": "/collect-log-cloud-watch/4_visualize_analyze.html",
	"title": "4. Visualize and Analyze",
	"tags": [],
	"description": "",
	"content": "OpenSearch provides us the ability to analyze out logs. Lets begin by naviaging to the OpenSearch Dashboard\nOpen the OpenSearch Dashboard  Go to the OpenSearch Console Click on the workshop-domain OpenSearch domain you created earlier  Click on the OpenSearch Dashboard URL. This should open the URL in a web browser window  You will be prompted to log in. Using the user name and password you created during the OpenSearch deployment, log in If an additonal pop up window is present after login asking about data upload click on Explore on my own If an additonal pop up windows is present asking you to select your tenant select Global and click on Confirm  You should now see a window that looks like this\nCreate an Index Pattern The Lambda functions the send the messages from CloudWatch to OpenSearch will create a new OpenSearch index each day. Each index name will start with cwl and will be followed by the date.\nTo search all of the CloudWatch logs (ie. multiple days) we will create an index pattern in OpenSearch. The index pattern will be a representation of all of the cwl log indexes for all of the days.\n In the OpenSeach Dashboard, expand the side menu and click on Stack Management under managment section  On the stack managment page click on Index Patterns on the left hand menu  On the index patterns page click on Create index pattern  Enter workshop-log-* under the index pattern name section Click on Next step  Select @timestamp as the primary time feild Click on Create index pattern  We have now created an index pattern! We can use the index pattern to analyze our logs\nSearch the Logs OpenSearch provides the ability to easily search log data. Lets run a few simple searches on our logs\n In the OpenSearch Dashboard expand the side menu and click on Discover under the OpenSearch Dashboards section  "
},
{
	"uri": "/open-search-log-analytics/4_visualize_analyze.html",
	"title": "4. Visualize and Analyze",
	"tags": [],
	"description": "",
	"content": "OpenSearch provides us the ability to analyze out logs. Lets begin by naviaging to the OpenSearch Dashboard\nOpen the OpenSearch Dashboard  Go to the OpenSearch Console Click on the workshop-domain OpenSearch domain you created earlier  Click on the OpenSearch Dashboard URL. This should open the URL in a web browser window  You will be prompted to log in. Using the user name and password you created during the OpenSearch deployment, log in If an additonal pop up window is present after login asking about data upload click on Explore on my own If an additonal pop up windows is present asking you to select your tenant select Global and click on Confirm  You should now see a window that looks like this\nCreate an Index Pattern When we deployed Kinesis Data Firehose we configured it to create a new index in OpenSearch every 1 hr. We also configured it to name each index starting with workshop-log\nThis means that open search will have 1 index for each hour it is sent logs and that these indexies\u0026rsquo;s name will start with workshop-log\nIn order to work with all of the logs (ie. multiple hours) we will create an index pattern in OpenSearch. The index pattern will be a representation of all of the workshop-log indexes for all of the hours\n In the OpenSeach Dashboard, expand the side menu and click on Stack Management under managment section  On the stack managment page click on Index Patterns on the left hand menu  On the index patterns page click on Create index pattern  Enter workshop-log-* under the index pattern name section Click on Next step  Click on Create index pattern  We have now created an index pattern! We can use the index pattern to analyze our logs\nSearch the Logs OpenSearch provides the ability to easily search log data. Lets run a few simple searches on our logs\n In the OpenSearch Dashboard expand the side menu and click on Discover under the OpenSearch Dashboards section  This will bring you to the discovery page. On this page we can see the log data we sent via. the Cloud9 Python application\nWe can now run a few different searched against our index pattern. To get started lets look for any log messages that are Hadoop Error. Remeber that we have 4 types of logs in our index Hadoop, Spark, HDFS, ZooKeeper\nRun the search \u0026quot;Hadoop\u0026quot; AND \u0026quot;Error\u0026quot;  OpenSearch displays the 156 Hadoop error logs of the total 8000 logs. Now that you have run a search. Try running at least 3 other searchs. A few search suggestions are below. However feel free to come up with you own\nSuggested searches\n \u0026quot;spark\u0026quot; AND \u0026quot;broadcast\u0026quot; \u0026quot;hdfs\u0026quot; \u0026quot;zoo_keeper\u0026quot; AND \u0026quot;WARN\u0026quot; AND \u0026quot;waiting for message\u0026quot;  After you have run a few searches. We can look at creating a visualization and dashboard\nCreate a Visualization  In the OpenSearch Dashboard expand the side menu and click on Visualize under the OpenSearch Dashboards section  Click on Create new visualization  Click on Gauge from the list of visualizations Click workshop-log- from the list of index patterns On the create page enter \u0026quot;Hadoop\u0026quot; AND \u0026quot;ERROR\u0026quot; in the search bar  This will produce a gauge chart visualizing the number of hadoop error logs.\nClick on the Save button. Name the visual anything you would like  Create a Dashboard Dashboards allow you to combine multiple visualizations in a single place. Lets build a simple dashboard\n In the OpenSearch Dashboard expand the side menu and click on Dashboards under the OpenSearch Dashboards section  On the dashboard page click on Create new dashboard  On the dashboard page click on Add an existing  Select the visualization you just created.  Use the create new button or repeat the earlier process to create a second visual and add it to your dashboard. Have at least 2 visuals on your dashboard.\nAlso take note of the ability to save and share this dashboard with other. These options are listed on the top bar\nWhen you are ready proceed to the next step Clean Up if you want to delete the resources we used for this workshop\n"
},
{
	"uri": "/collect-log-cloud-watch/5_clean_up.html",
	"title": "5. Clean Up",
	"tags": [],
	"description": "",
	"content": "Delete OpenSearch  Go to the OpenSearch Console Click on the workshop-domain OpenSearch domain you created earlier  Click on Delete  Follow the instructions in the prompt to complete the delete  Delete Glue Jobs Delete Cloud Watch Log Groups Delete Lambda Functions You have completed deleting the resources used in this workshop\n"
},
{
	"uri": "/open-search-log-analytics/5_clean_up.html",
	"title": "5. Clean Up",
	"tags": [],
	"description": "",
	"content": "Delete Cloud 9  Go to the Cloud9 Console  Select your Cloud9 enviorment Click on Delete Follow the instructions in the prompt to complete the delete  Delete Kinesis Data Firehose  Go to the Kinesis Firehose Console Click on the selector next to the Kinesis Data Firehose we created earlier  Click on Delete Follow the instructions in the prompt to complete the delete  Delete OpenSearch  Go to the OpenSearch Console Click on the workshop-domain OpenSearch domain you created earlier  Click on Delete  Follow the instructions in the prompt to complete the delete  You have completed deleting the resources used in this workshop\n"
},
{
	"uri": "/collect-log-cloud-watch/1_getting_started/aws_event.html",
	"title": "AWS Event",
	"tags": [],
	"description": "",
	"content": "\rOnly complete this section if you are at an AWS hosted event (such as re:Invent, Loft, Immersion Day, or any other event hosted by an AWS employee). If you are running the workshop on your own, go to Self Paced.\n\rLog into AWS Console via. AWS Workshop Portal Your instructor has already created an AWS account for you. Your instructor should provide you a participant hash.\nOnce you have your participant hash go to https://dashboard.eventengine.run/\nEnter your participant has and select Accept Terms \u0026amp; Login\nSelect how you want to log in. Selecting Email One-Time Password (OTP) is recommended. However you can also use your Amazon.com retail account to login.\nFollow the prompts to complete the login process. Once you have sucssfully loged in. You will see the following screen. Select AWS Console\nA window will open. From the window select Open AWS Console\nThis will open the AWS Console in a new window on your web browser. If you can see the home page for the AWS Console as depicted below you have sucssfully logged into your AWS account.\nNow that you have successfully logged into your AWS account and are able to access the AWS Console, lets begin the next step Environment Set Up\n"
},
{
	"uri": "/open-search-log-analytics/1_getting_started/aws_event.html",
	"title": "AWS Event",
	"tags": [],
	"description": "",
	"content": "\rOnly complete this section if you are at an AWS hosted event (such as re:Invent, Loft, Immersion Day, or any other event hosted by an AWS employee). If you are running the workshop on your own, go to Self Paced.\n\rLog into AWS Console via. AWS Workshop Portal Your instructor has already created an AWS account for you. Your instructor should provide you a participant hash.\nOnce you have your participant hash go to https://dashboard.eventengine.run/\nEnter your participant has and select Accept Terms \u0026amp; Login\nSelect how you want to log in. Selecting Email One-Time Password (OTP) is recommended. However you can also use your Amazon.com retail account to login.\nFollow the prompts to complete the login process. Once you have sucssfully loged in. You will see the following screen. Select AWS Console\nA window will open. From the window select Open AWS Console\nThis will open the AWS Console in a new window on your web browser. If you can see the home page for the AWS Console as depicted below you have sucssfully logged into your AWS account.\nNow that you have successfully logged into your AWS account and are able to access the AWS Console, lets begin the next step Environment Set Up\n"
},
{
	"uri": "/collect-log-cloud-watch/1_getting_started/self_paced.html",
	"title": "Self Paced",
	"tags": [],
	"description": "",
	"content": "\rOnly complete this section if you are running the workshop on your own. If you are at an AWS hosted event (such as re:Invent, Loft, Immersion Day, or any other event hosted by an AWS employee), continue with AWS Event.\n\rRunning the Workshop on Your Own You will need an AWS Account with web console access to complete the workshop.\nIf you do not have an AWS account Create an AWS Account\nAs pictured below you should be able to access the AWS Console\nOnce you have can access the AWS Console, lets begin the next step Enviorment Set Up\n"
},
{
	"uri": "/open-search-log-analytics/1_getting_started/self_paced.html",
	"title": "Self Paced",
	"tags": [],
	"description": "",
	"content": "\rOnly complete this section if you are running the workshop on your own. If you are at an AWS hosted event (such as re:Invent, Loft, Immersion Day, or any other event hosted by an AWS employee), continue with AWS Event.\n\rRunning the Workshop on Your Own You will need an AWS Account with web console access to complete the workshop.\nIf you do not have an AWS account Create an AWS Account\nAs pictured below you should be able to access the AWS Console\nOnce you have can access the AWS Console, lets begin the next step Enviorment Set Up\n"
},
{
	"uri": "/categories.html",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags.html",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]